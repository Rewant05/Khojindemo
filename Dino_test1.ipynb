{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11924023,"sourceType":"datasetVersion","datasetId":7496765},{"sourceId":11924207,"sourceType":"datasetVersion","datasetId":7496896},{"sourceId":11924560,"sourceType":"datasetVersion","datasetId":7497136}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision faiss-cpu open-clip-torch transformers scikit-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T16:12:27.678813Z","iopub.execute_input":"2025-05-23T16:12:27.679024Z","iopub.status.idle":"2025-05-23T16:13:44.131828Z","shell.execute_reply.started":"2025-05-23T16:12:27.679007Z","shell.execute_reply":"2025-05-23T16:13:44.130952Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nCollecting open-clip-torch\n  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\nCollecting ftfy (from open-clip-torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.31.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.15)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, open-clip-torch, faiss-cpu\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed faiss-cpu-1.11.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open-clip-torch-2.32.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Final","metadata":{}},{"cell_type":"code","source":"import torch\nimport open_clip\nimport faiss\nimport numpy as np\nfrom torchvision import transforms\nfrom torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\nfrom PIL import Image\nimport os\nfrom typing import List, Tuple, Optional, Union\nimport json\n\nclass HybridSearchSystem:\n    def __init__(self, device=None):\n        \"\"\"\n        Initialize the hybrid search system with CLIP and DINOv2 models\n        \"\"\"\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Load CLIP model\n        print(\"Loading CLIP model...\")\n        self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n            'ViT-B-32', pretrained='laion2b_s34b_b79k'\n        )\n        self.clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n        self.clip_model = self.clip_model.to(self.device)\n        self.clip_model.eval()\n        \n        # Load DINOv2 (using ViT as substitute)\n        print(\"Loading DINOv2 model...\")\n        self.dino_model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n        self.dino_model = self.dino_model.to(self.device)\n        self.dino_model.eval()\n        \n        # Initialize separate indices\n        self.text_index = None\n        self.image_index = None\n        self.hybrid_index = None\n        \n        # Initialize variables\n        self.image_paths = []\n        self.text_descriptions = []\n        self.id_map = []\n        \n        print(f\"Models loaded successfully on {self.device}\")\n    \n    def embed_text(self, text: str) -> torch.Tensor:\n        \"\"\"Embed text using CLIP\"\"\"\n        tokens = self.clip_tokenizer([text]).to(self.device)\n        with torch.no_grad():\n            embedding = self.clip_model.encode_text(tokens).squeeze().cpu()\n        return embedding / embedding.norm()  # Normalize\n    \n    def embed_clip_image(self, image_path: str) -> torch.Tensor:\n        \"\"\"Embed image using CLIP\"\"\"\n        try:\n            image = self.clip_preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                embedding = self.clip_model.encode_image(image).squeeze().cpu()\n            return embedding / embedding.norm()  # Normalize\n        except Exception as e:\n            print(f\"Error processing image {image_path}: {e}\")\n            return None\n    \n    def embed_dino_image(self, image_path: str) -> torch.Tensor:\n        \"\"\"Embed image using DINOv2 (ViT)\"\"\"\n        try:\n            weights = ViT_B_16_Weights.IMAGENET1K_V1\n            preprocess = weights.transforms()\n            image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                embedding = self.dino_model(image).squeeze().cpu()\n            return embedding / embedding.norm()  # Normalize\n        except Exception as e:\n            print(f\"Error processing image {image_path}: {e}\")\n            return None\n    \n    def embed_hybrid_image(self, image_path: str) -> torch.Tensor:\n        \"\"\"Create hybrid embedding combining CLIP and DINOv2\"\"\"\n        clip_vec = self.embed_clip_image(image_path)\n        dino_vec = self.embed_dino_image(image_path)\n        \n        if clip_vec is None or dino_vec is None:\n            return None\n            \n        # Concatenate embeddings\n        hybrid_vec = torch.cat([clip_vec, dino_vec])\n        return hybrid_vec / hybrid_vec.norm()  # Normalize\n    \n    def build_database(self, image_paths: List[str], text_descriptions: List[str]):\n        \"\"\"\n        Build separate search databases for different query types\n        \n        Args:\n            image_paths: List of paths to images\n            text_descriptions: List of text descriptions corresponding to images\n        \"\"\"\n        if len(image_paths) != len(text_descriptions):\n            raise ValueError(\"Number of images and descriptions must match\")\n        \n        self.image_paths = image_paths\n        self.text_descriptions = text_descriptions\n        self.id_map = list(range(len(image_paths)))\n        \n        print(f\"Building database with {len(image_paths)} items...\")\n        \n        # Storage for different types of embeddings\n        text_embeddings = []\n        image_embeddings = []\n        hybrid_embeddings = []\n        valid_indices = []\n        \n        for i, (img_path, desc) in enumerate(zip(image_paths, text_descriptions)):\n            print(f\"Processing item {i+1}/{len(image_paths)}: {os.path.basename(img_path)}\")\n            \n            # Get individual embeddings\n            img_embedding = self.embed_hybrid_image(img_path)\n            text_embedding = self.embed_text(desc)\n            \n            if img_embedding is None:\n                print(f\"Skipping item {i} due to image processing error\")\n                continue\n            \n            # Store embeddings for different indices\n            text_embeddings.append(text_embedding.numpy())\n            image_embeddings.append(img_embedding.numpy())\n            \n            # Create hybrid embedding (image + text)\n            hybrid_embedding = torch.cat([img_embedding, text_embedding])\n            hybrid_embedding = hybrid_embedding / hybrid_embedding.norm()\n            hybrid_embeddings.append(hybrid_embedding.numpy())\n            \n            valid_indices.append(i)\n        \n        if not text_embeddings:\n            raise ValueError(\"No valid embeddings could be created\")\n        \n        # Update lists to only include valid items\n        self.image_paths = [self.image_paths[i] for i in valid_indices]\n        self.text_descriptions = [self.text_descriptions[i] for i in valid_indices]\n        self.id_map = list(range(len(self.image_paths)))\n        \n        # Build separate FAISS indices\n        \n        # 1. Text-only index\n        text_embeddings_array = np.stack(text_embeddings).astype('float32')\n        text_dimension = text_embeddings_array.shape[1]\n        self.text_index = faiss.IndexFlatIP(text_dimension)\n        self.text_index.add(text_embeddings_array)\n        \n        # 2. Image-only index\n        image_embeddings_array = np.stack(image_embeddings).astype('float32')\n        image_dimension = image_embeddings_array.shape[1]\n        self.image_index = faiss.IndexFlatIP(image_dimension)\n        self.image_index.add(image_embeddings_array)\n        \n        # 3. Hybrid index\n        hybrid_embeddings_array = np.stack(hybrid_embeddings).astype('float32')\n        hybrid_dimension = hybrid_embeddings_array.shape[1]\n        self.hybrid_index = faiss.IndexFlatIP(hybrid_dimension)\n        self.hybrid_index.add(hybrid_embeddings_array)\n        \n        print(f\"Database built successfully with {len(text_embeddings)} items\")\n        print(f\"Text embedding dimension: {text_dimension}\")\n        print(f\"Image embedding dimension: {image_dimension}\")\n        print(f\"Hybrid embedding dimension: {hybrid_dimension}\")\n    \n    def search(self, image_path: Optional[str] = None, \n               text_query: Optional[str] = None, \n               k: int = 5, \n               similarity_threshold: float = 0.0) -> List[Tuple[str, str, float]]:\n        \"\"\"\n        Search for similar items using appropriate index based on query type\n        \n        Args:\n            image_path: Path to query image (optional)\n            text_query: Text query (optional)\n            k: Number of results to return\n            similarity_threshold: Minimum similarity score (0-1)\n            \n        Returns:\n            List of tuples (image_path, description, similarity_score)\n        \"\"\"\n        if not image_path and not text_query:\n            raise ValueError(\"Must provide either image_path or text_query\")\n        \n        if self.text_index is None:\n            raise ValueError(\"Database not built. Call build_database() first.\")\n        \n        # Route to appropriate index based on query type\n        if text_query and not image_path:\n            # Text-only search\n            print(f\"Processing query text: '{text_query}'\")\n            text_embedding = self.embed_text(text_query)\n            query_embedding = text_embedding.numpy().reshape(1, -1).astype('float32')\n            similarities, indices = self.text_index.search(query_embedding, k)\n            \n        elif image_path and not text_query:\n            # Image-only search\n            print(f\"Processing query image: {os.path.basename(image_path)}\")\n            img_embedding = self.embed_hybrid_image(image_path)\n            if img_embedding is None:\n                raise ValueError(\"Could not process query image\")\n            query_embedding = img_embedding.numpy().reshape(1, -1).astype('float32')\n            similarities, indices = self.image_index.search(query_embedding, k)\n            \n        else:\n            # Hybrid search (image + text)\n            print(f\"Processing hybrid query - Image: {os.path.basename(image_path)}, Text: '{text_query}'\")\n            img_embedding = self.embed_hybrid_image(image_path)\n            text_embedding = self.embed_text(text_query)\n            \n            if img_embedding is None:\n                raise ValueError(\"Could not process query image\")\n            \n            # Create hybrid query embedding\n            hybrid_embedding = torch.cat([img_embedding, text_embedding])\n            hybrid_embedding = hybrid_embedding / hybrid_embedding.norm()\n            query_embedding = hybrid_embedding.numpy().reshape(1, -1).astype('float32')\n            similarities, indices = self.hybrid_index.search(query_embedding, k)\n        \n        # Format results\n        results = []\n        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):\n            if similarity >= similarity_threshold:\n                results.append((\n                    self.image_paths[idx],\n                    self.text_descriptions[idx],\n                    float(similarity)\n                ))\n        \n        return results\n    \n    def find_best_match(self, image_path: Optional[str] = None, \n                       text_query: Optional[str] = None,\n                       similarity_threshold: float = 0.7) -> Optional[Tuple[str, str, float]]:\n        \"\"\"\n        Find the best match above the similarity threshold\n        \n        Args:\n            image_path: Path to query image (optional)\n            text_query: Text query (optional)\n            similarity_threshold: Minimum similarity score for a valid match\n            \n        Returns:\n            Tuple (image_path, description, similarity_score) or None if no good match\n        \"\"\"\n        results = self.search(image_path, text_query, k=1, similarity_threshold=similarity_threshold)\n        \n        if results:\n            return results[0]\n        else:\n            return None\n    \n    def save_database(self, filepath: str):\n        \"\"\"Save all databases to disk\"\"\"\n        if self.text_index is None:\n            raise ValueError(\"No database to save\")\n        \n        # Save FAISS indices\n        faiss.write_index(self.text_index, f\"{filepath}_text.faiss\")\n        faiss.write_index(self.image_index, f\"{filepath}_image.faiss\")\n        faiss.write_index(self.hybrid_index, f\"{filepath}_hybrid.faiss\")\n        \n        # Save metadata\n        metadata = {\n            'image_paths': self.image_paths,\n            'text_descriptions': self.text_descriptions,\n            'id_map': self.id_map\n        }\n        \n        with open(f\"{filepath}_metadata.json\", 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        print(f\"Database saved to {filepath}\")\n    \n    def load_database(self, filepath: str):\n        \"\"\"Load all databases from disk\"\"\"\n        # Load FAISS indices\n        self.text_index = faiss.read_index(f\"{filepath}_text.faiss\")\n        self.image_index = faiss.read_index(f\"{filepath}_image.faiss\")\n        self.hybrid_index = faiss.read_index(f\"{filepath}_hybrid.faiss\")\n        \n        # Load metadata\n        with open(f\"{filepath}_metadata.json\", 'r') as f:\n            metadata = json.load(f)\n        \n        self.image_paths = metadata['image_paths']\n        self.text_descriptions = metadata['text_descriptions']\n        self.id_map = metadata['id_map']\n        \n        print(f\"Database loaded from {filepath}\")\n\n\n# Example usage\ndef main():\n    # Initialize the search system\n    search_system = HybridSearchSystem()\n    \n    # Example database - replace with your actual paths and descriptions\n    image_paths = [\n        '/kaggle/input/images/image1.jpg',\n        '/kaggle/input/images/image2.jpeg', \n        '/kaggle/input/images/image3.png',\n        '/kaggle/input/images2/image4.jpg'\n    ]\n    \n    text_descriptions = [\n        'red sports car in parking lot',\n        'blue ocean waves crashing on beach',\n        'historic stone building with arched windows',\n        'blue travel suitcase with front pocket'\n    ]\n    \n    # Build the database\n    try:\n        search_system.build_database(image_paths, text_descriptions)\n        \n        # Example queries\n        print(\"\\n\" + \"=\"*50)\n        print(\"SEARCH EXAMPLES\")\n        print(\"=\"*50)\n        \n        # Query with text only\n        print(\"\\n1. Text-only query:\")\n        results = search_system.search(text_query=\"blue suitcase\", k=3)\n        for i, (img_path, desc, score) in enumerate(results, 1):\n            print(f\"  {i}. {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        \n        # Query with image only\n        print(\"\\n2. Image-only query:\")\n        query_image = \"/kaggle/input/images2/query.jpg\"  # Replace with actual path\n        if os.path.exists(query_image):\n            results = search_system.search(image_path=query_image, k=3)\n            for i, (img_path, desc, score) in enumerate(results, 1):\n                print(f\"  {i}. {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        \n        # Query with both image and text\n        print(\"\\n3. Hybrid query (image + text):\")\n        if os.path.exists(query_image):\n            results = search_system.search(\n                image_path=query_image, \n                text_query=\"travel luggage\", \n                k=3\n            )\n            for i, (img_path, desc, score) in enumerate(results, 1):\n                print(f\"  {i}. {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        \n        # Find best match with threshold\n        print(\"\\n4. Best match with threshold:\")\n        best_match = search_system.find_best_match(\n            text_query=\"blue suitcase\", \n            similarity_threshold=0.7\n        )\n        \n        if best_match:\n            img_path, desc, score = best_match\n            print(f\"  Best match: {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        else:\n            print(\"  No good match found above threshold\")\n        \n        # Save database (optional)\n        # search_system.save_database(\"my_search_database\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:07:21.085663Z","iopub.execute_input":"2025-05-23T17:07:21.086165Z","iopub.status.idle":"2025-05-23T17:07:24.906150Z","shell.execute_reply.started":"2025-05-23T17:07:21.086135Z","shell.execute_reply":"2025-05-23T17:07:24.905585Z"}},"outputs":[{"name":"stdout","text":"Loading CLIP model...\nLoading DINOv2 model...\nModels loaded successfully on cuda\nBuilding database with 4 items...\nProcessing item 1/4: image1.jpg\nProcessing item 2/4: image2.jpeg\nProcessing item 3/4: image3.png\nProcessing item 4/4: image4.jpg\nDatabase built successfully with 4 items\nText embedding dimension: 512\nImage embedding dimension: 1512\nHybrid embedding dimension: 2024\n\n==================================================\nSEARCH EXAMPLES\n==================================================\n\n1. Text-only query:\nProcessing query text: 'blue suitcase'\n  1. image4.jpg - blue travel suitcase with front pocket (Score: 0.922)\n  2. image2.jpeg - blue ocean waves crashing on beach (Score: 0.535)\n  3. image3.png - historic stone building with arched windows (Score: 0.366)\n\n2. Image-only query:\nProcessing query image: query.jpg\n  1. image4.jpg - blue travel suitcase with front pocket (Score: 0.832)\n  2. image3.png - historic stone building with arched windows (Score: 0.145)\n  3. image1.jpg - red sports car in parking lot (Score: 0.053)\n\n3. Hybrid query (image + text):\nProcessing hybrid query - Image: query.jpg, Text: 'travel luggage'\n  1. image4.jpg - blue travel suitcase with front pocket (Score: 0.823)\n  2. image3.png - historic stone building with arched windows (Score: 0.273)\n  3. image2.jpeg - blue ocean waves crashing on beach (Score: 0.220)\n\n4. Best match with threshold:\nProcessing query text: 'blue suitcase'\n  Best match: image4.jpg - blue travel suitcase with front pocket (Score: 0.922)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Example usage\ndef main():\n    # Initialize the search system\n    search_system = HybridSearchSystem()\n    \n    # Example database - replace with your actual paths and descriptions\n    image_paths = [\n        '/kaggle/input/images/image1.jpg',\n        '/kaggle/input/images/image2.jpeg', \n        '/kaggle/input/images/image3.png',\n        '/kaggle/input/images2/image4.jpg'\n    ]\n    \n    text_descriptions = [\n        'red sports car in parking lot',\n        'blue ocean waves crashing on beach',\n        'historic stone building with arched windows',\n        'blue travel suitcase with front pocket'\n    ]\n    \n    # Build the database\n    try:\n        search_system.build_database(image_paths, text_descriptions)\n        \n        # Example queries\n        print(\"\\n\" + \"=\"*50)\n        print(\"SEARCH EXAMPLES\")\n        print(\"=\"*50)\n        \n        # Query with text only\n        print(\"\\n1. Text-only query:\")\n        results = search_system.search(text_query=\"grey suitcase\", k=3)\n        for i, (img_path, desc, score) in enumerate(results, 1):\n            print(f\"  {i}. {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        \n        # Query with image only\n        print(\"\\n2. Image-only query:\")\n        query_image = \"/kaggle/input/dinodino/q2.jpg\"  # Replace with actual path\n        if os.path.exists(query_image):\n            results = search_system.search(image_path=query_image, k=3)\n            for i, (img_path, desc, score) in enumerate(results, 1):\n                print(f\"  {i}. {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        \n        # Query with both image and text\n        print(\"\\n3. Hybrid query (image + text):\")\n        if os.path.exists(query_image):\n            results = search_system.search(\n                image_path=query_image, \n                text_query=\"travel luggage\", \n                k=3\n            )\n            for i, (img_path, desc, score) in enumerate(results, 1):\n                print(f\"  {i}. {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        \n        # Find best match with threshold\n        print(\"\\n4. Best match with threshold:\")\n        best_match = search_system.find_best_match(\n            image_path=query_image, \n            text_query=\"travel luggage\", \n            similarity_threshold=0.8\n        )\n        \n        if best_match:\n            img_path, desc, score = best_match\n            print(f\"  Best match: {os.path.basename(img_path)} - {desc} (Score: {score:.3f})\")\n        else:\n            print(\"  No good match found above threshold\")\n        \n        # Save database (optional)\n        # search_system.save_database(\"my_search_database\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:19:50.002385Z","iopub.execute_input":"2025-05-23T17:19:50.002994Z","iopub.status.idle":"2025-05-23T17:19:53.682328Z","shell.execute_reply.started":"2025-05-23T17:19:50.002972Z","shell.execute_reply":"2025-05-23T17:19:53.681565Z"}},"outputs":[{"name":"stdout","text":"Loading CLIP model...\nLoading DINOv2 model...\nModels loaded successfully on cuda\nBuilding database with 4 items...\nProcessing item 1/4: image1.jpg\nProcessing item 2/4: image2.jpeg\nProcessing item 3/4: image3.png\nProcessing item 4/4: image4.jpg\nDatabase built successfully with 4 items\nText embedding dimension: 512\nImage embedding dimension: 1512\nHybrid embedding dimension: 2024\n\n==================================================\nSEARCH EXAMPLES\n==================================================\n\n1. Text-only query:\nProcessing query text: 'grey suitcase'\n  1. image4.jpg - blue travel suitcase with front pocket (Score: 0.783)\n  2. image3.png - historic stone building with arched windows (Score: 0.412)\n  3. image2.jpeg - blue ocean waves crashing on beach (Score: 0.366)\n\n2. Image-only query:\nProcessing query image: q2.jpg\n  1. image4.jpg - blue travel suitcase with front pocket (Score: 0.717)\n  2. image1.jpg - red sports car in parking lot (Score: 0.063)\n  3. image2.jpeg - blue ocean waves crashing on beach (Score: 0.054)\n\n3. Hybrid query (image + text):\nProcessing hybrid query - Image: q2.jpg, Text: 'travel luggage'\n  1. image4.jpg - blue travel suitcase with front pocket (Score: 0.765)\n  2. image3.png - historic stone building with arched windows (Score: 0.225)\n  3. image2.jpeg - blue ocean waves crashing on beach (Score: 0.220)\n\n4. Best match with threshold:\nProcessing hybrid query - Image: q2.jpg, Text: 'travel luggage'\n  No good match found above threshold\n","output_type":"stream"}],"execution_count":46}]}